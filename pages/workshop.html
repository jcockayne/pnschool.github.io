<!DOCTYPE html>
<html>


<head>
    <title>ProbNum School 2023</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <link rel="stylesheet" type="text/css" href="../style.css">
</head>

<header id="header">
    <h1>
        Probabilistic Numerics Spring School
        <br>
        <i>Southampton 2024</i>
      </h1>
      <h2>April 8 - 10, 2024</h2>


    <nav>
        <ul>
            <li><a href="home.html#navBar" class='active-page'>Home</a></li>
            <li><a href="registration.html#navBar">Registration</a></li>
            <li><a href="school.html#navBar">School</a></li>
            <li><a href="workshop.html#navBar">Workshop</a></li>
            <li><a href="schedule.html#navBar">Schedule</a></li>
            <li><a href="faq.html#navBar">FAQ</a></li>
        </ul>
    </nav>
</header>


<body>
    <main>
        <a id="navBar"> </a>
        <h1>Workshop</h1>

        <p>
            The workshop will take place on the 10th of April, 2024. It's a one-day event
            with talks and discussions revolving around probabilistic numerical methods. The workshop is open to all
            <a href="registration.html#navBar">registered</a> participants.
        </p>



        <h2>Confirmed Speakers </h2>

        <table>

            <tr>
                <td style="width:20%">
                    <img src=" ../images/dummy_person.jpg" div class="round" align="left" />
                </td>
                <td>
                <h3>
                    Han Cheng Lie
                </h3>
                </h3>
                <i>
                    Universit√§t Potsdam
                </i>
                <span class="right">
                    <a href="https://sites.google.com/site/hanchenglie"
                    target="_blank">
                    &#10132; Website
                    </a>
                </span>
                </td>
            </tr>

           <tr>
            <td style="width:20%">
                <img src=" ../images/FrederikDeCeuster_stolen.png" div class="round" align="left" />
            </td>
            <td>
            <h3>
                Frederik De Ceuster
            </h3>
            </h3>
            <i>
            Institute of Astronomy (KU Leuven)
            </i>
            <span class="right">
                <a href="https://freddeceuster.github.io/"
                target="_blank">
                &#10132; Website
                </a>
            </span>
            </td>
            </tr>

            <tr>
                <td style="width:20%">
                  <img src=" ../images/disha.jpg" div class="round" align="top" />
                </td>
                <td>
                <h3>
                  Disha Hegde
                </h3>
              </h3>
              <i>
                University of Southampton
              </i>
                </td> </tr>
        </table>

                <p>
                  <b>Learning to Solve Related Linear Systems</b> <br>
                  <i>
                    Solving multiple large linear systems defined across a parameter space is central to many numerical tasks, such as solving nonlinear PDEs arising from applications like fluid dynamics, optimising hyperparameters of Gaussian processes and finding coefficients for statistical models. The computational expense of solving these linear systems can be lowered if their interdependence across the parameter space is exploited efficiently. This talk extends the idea of probabilistic linear solvers across a space of parameters. This probabilistic solver is used as a companion with standard iterative solvers like the Conjugate Gradient method to provide an efficient initial guess and preconditioner, that accelerate the convergence.
                  </i> 
                </p> 
              
        <table>
            <tr>
                <td style="width:20%">
                    <img src=" ../images/Motonobu_Kanagawa_stolen.png" div class="round" align="left" />
                </td>
                <td>
                <h3>
                    Motonobu Kanagawa
                </h3>
                </h3>
                <i>
                Eurecom
                <span class="right">
                    <a href="https://sites.google.com/site/motonobukanagawa/"
                    target="_blank">
                    &#10132; Website
                    </a>
                </span>
              </td>
            </tr>
              </table>
    
              <p>
              </i>
              <b>Comparing Scale Parameter Estimators for Gaussian Process Regression: Cross Validation and Maximum Likelihood </b> <br>
              <i>
                  Gaussian process (GP) regression is a Bayesian nonparametric method for regression and interpolation, offering a principled way of quantifying the uncertainties of predicted function values. 
                  For the quantified uncertainties to be well-calibrated, however, the covariance kernel of the GP prior has to be carefully selected. 
                  In this talk, we theoretically compare two methods for choosing the kernel in GP regression: cross-validation and maximum likelihood estimation. 
                  Focusing on the scale-parameter estimation of a Brownian motion kernel in the noiseless setting, we prove that cross-validation can yield asymptotically well-calibrated credible intervals for a broader class of ground-truth functions than maximum likelihood estimation, suggesting an advantage of the former over the latter. 
                  (Joint work with Masha Naslidnyk, Toni Karvonen, and Maren Mahsereci)
              </i>
              </p>
            
            <table>
            <tr>
                <td style="width:20%">
                    <img src=" ../images/Katharina_Ott_stolen.jpg" div class="round" align="left" />
                </td>
                <td>
                <h3>
                    Katharina Ott
                </h3>
                </h3>
                <i>
                    University of T&uuml;bingen
                </i>
                </td>
            </tr>

            <tr>
              <td style="width:20%">
                  <img src=" ../images/Paz_stolen.jpg" div class="round" align="left" />
              </td>
              <td>
              <h3>
                  Paz Fink Shustin
              </h3>
              </h3>
              <i>
                  University of Oxford
              </i>
              <span class="right">
                  <a href="https://www.queens.ox.ac.uk/people/dr-paz-fink-shustin/"
                  target="_blank">
                  &#10132; Website
                  </a>
              </span>
              </td>
          </tr>


        </table>
        <p>
        </i>
        <b>Scalable Gaussian Process Regression with Gauss-Legendre Features</b> <br>
        <i>
          Gaussian processes provide a powerful probabilistic kernel learning framework, facilitating high-quality nonparametric learning via methods such as Gaussian process regression. Nevertheless, its learning phase requires unrealistic massive computations for large datasets. In this talk, we present a quadrature-based approach for scaling up Gaussian process regression via a low-rank approximation of the kernel matrix. Leveraging this low-rank structure enables us to achieve effective hyperparameter learning, training, and prediction. Our method is inspired by the well-known random Fourier features approach, which also builds low-rank approximations via numerical integration. However, our method is capable of generating high-quality kernel approximation using a number of features that is poly-logarithmic in the number of training points, while similar guarantees will require an amount that is at the very least linear in the number of training points when using random Fourier features. The utility of our method for learning with low-dimensional datasets is demonstrated through numerical experiments.
        </i>
        </p>









    <!--

    <h2>Schedule</h2>

    <table class="center">
      <tr>
        <th>Time</th>
        <th>
          Wednesday, 29.03.
        </th>
        <th style="text-align: left">
          Title
        </th>
      </tr>
      <tr>
        <td style="text-align:center">9:00-9:45</td>
        <td style="text-align:center"> Carl Henrik Ek</td>
        <td style="text-align:left">Modulated Surrogates for Bayesian Optimisation </td>
      </tr>
      <tr>
        <td style="text-align:center">9:45-10:30</td>
        <td style="text-align:center"> Fatemeh Yaghoobi</td>
        <td style="text-align:left">Parallel square-root statistical linear regression for state estimation in nonlinear state space models</td>
      </tr>
      <tr>
        <td style="text-align:center">10:30-11:00</td>
        <td style="text-align:center"> <i> Coffee break </i> </td>
        <td style="text-align:left">-------</td>
      </tr>
      <tr>
        <td style="text-align:center">11:00-11:45</td>
        <td style="text-align:center"> Jon Cockayne</td>
        <td style="text-align:left">A Bayesian Conjugate Gradient Method</td>
      </tr>
      <tr>
        <td style="text-align:center">11:45-13:00</td>
        <td style="text-align:center"> <i> Lunch break </i> </td>
        <td style="text-align:left">-------</td>
      </tr>
      <tr>
        <td style="text-align:center">13:00-13:45</td>
        <td style="text-align:center"> Masha Naslidnyk</td>
        <td style="text-align:left">Robust Estimation for Gaussian Processes and Beyond</td>
      </tr>
      <tr>
        <td style="text-align:center">13:45-14:30</td>
        <td style="text-align:center"> Toni Karvonen</td>
        <td style="text-align:left">Two pitfalls in Gaussian process interpolation</td>
      </tr>
      <tr>
        <td style="text-align:center">14:30-15:00</td>
        <td style="text-align:center"> <i> Coffee break </i> </td>
        <td style="text-align:left">-------</td>
      </tr>
      <tr>
        <td style="text-align:center">15:00-15:45</td>
        <td style="text-align:center"> Emilia Magnani</td>
        <td style="text-align:left">Learning solution operators for PDEs with uncertainty</td>
      </tr>
    </table>
    -->

  </main>










    </main>
    
    <footer>
        <p> &copy; ProbNum School Organisers, 2024 </p>
    </footer>

</body>

</html>
